{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperTokenizer, ViTModel\n",
    "import evaluate\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#############################################\n",
    "########### 변수 설정 #########################\n",
    "#############################################\n",
    "\n",
    "MAX_STEPS = 50\n",
    "LOG_INTERVER = 1\n",
    "BATCH_SIZE = 5  # 한 번에 4개의 음성 파일을 처리하도록 설정\n",
    "\n",
    "# EPSILON = 0.0001 # SPSA Perturbation 크기\n",
    "# ALPHA = 0.602 # SPSA Learning rate scaling 0.602\n",
    "# GAMMA = 0.101  # SPSA Decay 0.101\n",
    "# AK=0.0004 \n",
    "# CK=0.000025 # gradient 추정시 사용 값\n",
    "# O=10\n",
    "# P_TRIGGER_EPSILON = 0.00000005 # p_trigger 업데이트시 사용\n",
    "\n",
    "EPSILON = 0.001  # 기존보다 10배 증가\n",
    "ALPHA = 0.602\n",
    "GAMMA = 0.101\n",
    "AK = 0.00001  # 기존보다 25배 증가\n",
    "CK = 0.005  # 기존보다 5배 증가\n",
    "O = 7  # 기존보다 감소\n",
    "P_TRIGGER_EPSILON = 0.0000001  # 기존보다 10배 증가\n",
    "\n",
    "MAX_GRAD = 3000 # Gradient clipping 제한치\n",
    "\n",
    "\n",
    "# LOSS_FN = \"wer\"  # WER 기반 Loss\n",
    "LOSS_FN = \"cross entropy\"  # Loss 유형\n",
    "MAX_FRAMES = 3000\n",
    "meshgrid_HIDDEN_DIM = 768  # ViT hidden_dim\n",
    "MAX_NEW_TOKENS = 444  # Whisper default Max Tokens 448 - 4. 4: decoder_input_ids 개수\n",
    "ENCODER_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# whisper_version = \"openai/whisper-large-v3\"\n",
    "whisper_version = \"openai/whisper-small\"\n",
    "\n",
    "if whisper_version == \"openai/whisper-small\":\n",
    "    NUM_MEL_BINS = 80\n",
    "elif whisper_version == \"openai/whisper-large-v3\":\n",
    "    NUM_MEL_BINS = 128\n",
    "else:\n",
    "    raise ValueError(\"위스퍼 버전 확인 필요\")\n",
    "\n",
    "#############################################\n",
    "#############################################\n",
    "\n",
    "\n",
    "#############################################\n",
    "#####  MeshGridMask (Binary Mask) #####\n",
    "#############################################\n",
    "\n",
    "# ✅ **MeshGridMask: 필터 생성 (0 또는 1)**\n",
    "class MeshGridMask(nn.Module):\n",
    "    def __init__(self, mel_bins, frames):\n",
    "        super(MeshGridMask, self).__init__()\n",
    "        # 필터는 (mel_bins, frames) 크기이며 학습 가능한 파라미터로 설정됨\n",
    "        self.filter = nn.Parameter(torch.randint(0, 2, (mel_bins, frames), dtype=torch.float32), requires_grad=False)  # 0 또는 1 초기값\n",
    "\n",
    "    def forward(self, mel):\n",
    "        \"\"\"\n",
    "        mel: (batch_size, mel_bins, frames)\n",
    "        filter: (mel_bins, frames) → 확장하여 배치 차원 적용\n",
    "        \"\"\"\n",
    "        return mel * self.filter  # 필터링된 mel spectrogram\n",
    "\n",
    "############################\n",
    "\n",
    "# meshgrid 초기화\n",
    "meshgrid = MeshGridMask(mel_bins=NUM_MEL_BINS, frames=MAX_FRAMES).to(device)\n",
    "\n",
    "\n",
    "############################\n",
    "###### 업데이트 로직 설정 ######\n",
    "############################\n",
    "\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "normalizer = EnglishTextNormalizer() # normalizer 적용\n",
    "\n",
    "# def calculate_wer(references, predictions): \n",
    "#     return wer_metric.compute(references=references, predictions=predictions)\n",
    "\n",
    "def calculate_wer(references, predictions, tokenizer):\n",
    "    \"\"\"\n",
    "    패딩된 labels를 무시하고 WER을 계산하는 함수\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Gradient 저장 방지\n",
    "        filtered_references = []\n",
    "        \n",
    "        # 패딩을 제외한 원본 labels 추출 & list of characters → list of words 변환\n",
    "        for ref in references:\n",
    "            ref_filtered = [word for word in ref if word != tokenizer.pad_token_id]\n",
    "            filtered_references.append(\"\".join(ref_filtered))  # 🔹 join()을 사용해 문자 리스트를 문자열로 변환\n",
    "\n",
    "        # ✅ 리스트의 각 요소에 `normalizer()` 적용\n",
    "        filtered_references = [\"\".join(ref_filtered) for ref_filtered in filtered_references]  # 리스트 -> 문자열 변환\n",
    "        filtered_references = [normalizer(ref) for ref in filtered_references]  # 정상화 적용\n",
    "        predictions = [normalizer(pred) for pred in predictions]  # 정상화 적용\n",
    "\n",
    "    # ✅ 정상화된 데이터를 WER 계산에 사용\n",
    "    return wer_metric.compute(references=filtered_references, predictions=predictions)\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_cross_entropy_loss(whisper_model, mel_with_delta, labels):\n",
    "    \"\"\"\n",
    "    mel_with_delta: meshgrid와 결합된 Mel Spectrogram\n",
    "    labels: ground truth token IDs\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Gradient 저장 방지\n",
    "        outputs = whisper_model(input_features=mel_with_delta, labels=labels)\n",
    "    return outputs.loss  # CrossEntropy loss\n",
    "\n",
    "# SPSA 업데이트 함수 (Trigger Vector + Decoder 학습)\n",
    "# 디코더 전체를 벡터화하여 perturbation 후, 다시 벡터에서 파라미터로 변환\n",
    "\n",
    "\n",
    "class SPSA:\n",
    "    def __init__(self, epsilon=0.01, epsilon_decay=0.99, min_epsilon=0.0001):\n",
    "        \"\"\"\n",
    "        epsilon: 초기 perturbation 크기\n",
    "        epsilon_decay: 매 step마다 epsilon을 줄이는 감쇠율 (0 < epsilon_decay < 1)\n",
    "        min_epsilon: epsilon의 최소값 (너무 작아지지 않도록 제한)\n",
    "        \"\"\"\n",
    "        super(SPSA, self).__init__()   \n",
    "        self.epsilon_0 = epsilon\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.step_count = 0  # 현재 스텝\n",
    "\n",
    "    def parameter_update(self):\n",
    "        \"\"\" epsilon을 감소시키는 업데이트 \"\"\"\n",
    "        self.step_count += 1\n",
    "        self.epsilon = max(self.epsilon_0 * (self.epsilon_decay ** self.step_count), self.min_epsilon)\n",
    "\n",
    "    def spsa_update(self, meshgrid, whisper_model, mel: list, labels: list):\n",
    " \n",
    "        torch.cuda.empty_cache()  # 메모리 캐시 정리\n",
    "      \n",
    "        # **1. 필터 파라미터를 가져오기**\n",
    "        filter_params = meshgrid.filter.detach().clone()  # ✅ meshgrid.filter에서 직접 복사                \n",
    "\n",
    "\n",
    "        # Perturbation 방식 변경 (일부 값만 0↔1로 변경)\n",
    "        num_to_flip = int(self.epsilon * filter_params.numel())  # 업데이트할 개수 결정\n",
    "        \n",
    "        # 서로 다른 perturbation을 적용하도록 두 개의 indices 생성\n",
    "        indices_1 = torch.randint(0, meshgrid_params.numel(), (num_to_flip,), device=device)\n",
    "        indices_2 = torch.randint(0, meshgrid_params.numel(), (num_to_flip,), device=device)\n",
    "\n",
    "        # 필터 1 적용\n",
    "        filter_1 = meshgrid_params.clone()\n",
    "        filter_1.view(-1)[indices_1] = 1 - filter_1.view(-1)[indices_1]  # 기존 값 반전\n",
    "        mel_filtered_1 = mel * filter_1.unsqueeze(0)\n",
    "\n",
    "        # 필터 2 적용\n",
    "        filter_2 = meshgrid_params.clone()\n",
    "        filter_2.view(-1)[indices_2] = 1 - filter_2.view(-1)[indices_2]  # 기존 값 반전\n",
    "        mel_filtered_2 = mel * filter_2.unsqueeze(0)\n",
    "\n",
    "        predictions_1 = whisper_model.generate(input_features=mel_filtered_1, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        predictions_2 = whisper_model.generate(input_features=mel_filtered_2, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "        ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        pred_1_texts = tokenizer.batch_decode(predictions_1, skip_special_tokens=True)\n",
    "        pred_2_texts = tokenizer.batch_decode(predictions_2, skip_special_tokens=True)\n",
    "\n",
    "        print(\"Ref\", ref_texts)\n",
    "        print(\"Pred_1\",pred_1_texts)\n",
    "        print(\"Pred_2\",pred_2_texts)\n",
    "\n",
    "\n",
    "        if LOSS_FN == \"wer\":\n",
    "\n",
    "            loss_1 = calculate_wer(ref_texts, pred_1_texts, tokenizer) \n",
    "            loss_2 = calculate_wer(ref_texts, pred_2_texts, tokenizer) \n",
    "\n",
    "        elif LOSS_FN == \"cross entropy\":\n",
    "\n",
    "            loss_1 = calculate_cross_entropy_loss(whisper_model, mel_filtered_1, labels)\n",
    "            loss_2 = calculate_cross_entropy_loss(whisper_model, mel_filtered_2, labels)\n",
    "        else:\n",
    "            raise ValueError(\"Loss function not supported\")\n",
    "\n",
    "        if loss_1 < loss_2:\n",
    "            meshgrid.filter.data.copy_(filter_1)  # Gradient tracking 유지\n",
    "        else:\n",
    "            meshgrid.filter.data.copy_(filter_2)  # Gradient tracking 유지\n",
    "\n",
    "        return loss_1, loss_2\n",
    "\n",
    "\n",
    "#########################################\n",
    "####### Whisper 모델 로드 및 Freeze ########\n",
    "#########################################\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(whisper_version)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_version).to(device)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(whisper_version, language=\"en\", task=\"transcribe\")\n",
    "\n",
    " # Whisper 모델 Freeze\n",
    "for param in whisper_model.parameters():\n",
    "    param.requires_grad = False \n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "######### 데이터 준비 및 모델 초기화 #########\n",
    "#######################################\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_ID = \"Jzuluaga/atcosim_corpus\"\n",
    "dataset = load_dataset(DATASET_ID, \"default\", split=\"train[:2%]\")  # 데이터 일부만 사용\n",
    "\n",
    "# 데이터 전처리\n",
    "def preprocess_data(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    mel = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    labels = processor.tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=\"longest\").input_ids.squeeze(0)\n",
    "    return {\"mel\": torch.tensor(mel, dtype=torch.float32).unsqueeze(0).to(device), \"labels\": labels.to(device)}\n",
    "\n",
    "    \n",
    "processed_dataset = [preprocess_data(item) for item in dataset]\n",
    "\n",
    "# CustomDataset 및 DataLoader 정의\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "\n",
    "# ✅ collate_fn 정의 → 다른 길이의 labels 처리\n",
    "def collate_fn(batch):\n",
    "    mel_batch = torch.stack([item[\"mel\"].squeeze(0) for item in batch])  # Mel-Spectrogram 배치화\n",
    "    labels_batch = [item[\"labels\"] for item in batch]  # Labels 리스트로 유지\n",
    "\n",
    "    # ✅ 가장 긴 labels에 맞게 패딩\n",
    "    labels_padded = pad_sequence(labels_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return {\"mel\": mel_batch, \"labels\": labels_padded}\n",
    "\n",
    "# ✅ DataLoader 적용 (배치 크기 지정)\n",
    "dataset = CustomDataset(processed_dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)  # ✅ collate_fn 추가\n",
    "\n",
    "###################\n",
    "##### 학습 루프 #####\n",
    "###################\n",
    "\n",
    "print(\"Update method: \", LOSS_FN)\n",
    "spsa = SPSA(alpha=ALPHA, gamma=GAMMA, epsilon=EPSILON, ak=AK, ck=CK, o=O, p_trigger_epsilon=P_TRIGGER_EPSILON)\n",
    "avg_losses = []\n",
    "\n",
    "for epoch in range(MAX_STEPS):  # MAX_STEPS 만큼 반복\n",
    "    total_loss = 0.0 \n",
    "    num_batches = 0  # 배치 개수 카운트\n",
    "\n",
    "    for batch in dataloader:\n",
    "        mel = batch[\"mel\"].to(device)  # 배치 데이터를 텐서로 변환\n",
    "        labels = batch[\"labels\"].to(device)  # 패딩된 labels 텐서 변환\n",
    "\n",
    "        loss_1, loss_2 = spsa.spsa_update(meshgrid, whisper_model, mel, labels)\n",
    "\n",
    "        loss = min(loss_1, loss_2)  # 더 작은 Loss 선택\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "\n",
    "        spsa.parameter_update()  # epsilon 감소\n",
    "\n",
    "    # 해당 에폭의 평균 Loss 계산\n",
    "    avg_loss = total_loss / (num_batches * BATCH_SIZE)\n",
    "    avg_losses.append(avg_loss)\n",
    "\n",
    "    print()\n",
    "    print(f\"Epoch {epoch}: Avg WER Loss = {avg_loss:.4f}--------------------%%%%%%%%%%%%%%%%%%%@@@@@@@@@@@@@@@+++++++++++++++++++++\")\n",
    "    print()\n",
    "\n",
    "    if epoch >= MAX_STEPS:\n",
    "        break\n",
    "\n",
    "# 학습 완료 후 모델 저장\n",
    "torch.save(meshgrid.state_dict(), \"meshgrid.pth\")\n",
    "print(\"meshgrid saved!\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GPU 텐서를 CPU로 옮긴 후 NumPy 배열로 변환\n",
    "avg_losses_cpu = [loss.cpu().item() if isinstance(loss, torch.Tensor) else loss for loss in avg_losses]\n",
    "\n",
    "plt.plot(avg_losses_cpu)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(f'Average {LOSS_FN}')\n",
    "plt.title(f'Average {LOSS_FN} Over Epochs')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
