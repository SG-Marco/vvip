# audio_prompting_whisper.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import WhisperForConditionalGeneration
from audio_prompter import AudioPrompter

class AudioPromptingWhisper(nn.Module):
    """
    WhisperForConditionalGenerationì„ ê·¸ëŒ€ë¡œ ê°ì‹¸ê³ ,
    forward()ì—ì„œ 'input_features' ìœ„ì— audio_prompterë¡œë¶€í„° ë‚˜ì˜¨ deltaë¥¼ ë”í•´ ì¤Œ.
    """
    def __init__(self, base_model_name="openai/whisper-large-v3", max_frames=3000):
        super().__init__()
        # 1) Whisper ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
        self.whisper = WhisperForConditionalGeneration.from_pretrained(base_model_name)
        # Whisper ì‚¬ì „í•™ìŠµ íŒŒë¼ë¯¸í„° ë™ê²°
        for param in self.whisper.parameters():
            param.requires_grad = False
        # AudioPrompterëŠ” requires_grad=True

        # 2) AudioPrompter ì´ˆê¸°í™” (num_mel_bins=80 ê³ ì •, frames=3000 ê°€ì •)
        self.prompter = AudioPrompter(num_mel_bins=80, max_frames=max_frames)


    def forward(self, input_features, labels=None, **kwargs):
        """
        - ğŸ¤—Transformers TrainerëŠ” model(**batch) í˜•íƒœë¡œ í˜¸ì¶œ.
        - batch ë”•ì…”ë„ˆë¦¬ì—ëŠ” "input_features", "labels" ë“±ì´ ë“¤ì–´ìˆìŒ.
        - ì—¬ê¸°ì„œ input_features shape: (batch_size, 80, frames)
        """
        # 1) ì˜¤ë””ì˜¤ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        #    Whisper expects shape: (batch, 80, frames)
        #    self.prompterëŠ” (B,80,frames)ì— deltaë¥¼ ë”í•´ì¤Œ
        prompted_features = self.prompter(input_features)

        # 2) Whisper forward
        #    WhisperForConditionalGenerationì€
        #    forward(input_features=(B,80,frames), labels=(B, seq_len)) ì‹
        result = self.whisper(
            input_features=prompted_features,
            labels=labels,
            **kwargs
        )

        return result