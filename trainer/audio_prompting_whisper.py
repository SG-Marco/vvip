# VVIP/trainers/audio_prompting_whisper.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import WhisperForConditionalGeneration
from audio_prompters import AudioPrompter

# class AudioPromptingWhisper(nn.Module):
#     """
#     WhisperForConditionalGenerationì„ ê·¸ëŒ€ë¡œ ê°ì‹¸ì„œ,
#     forward()ì—ì„œ 'input_features' ìœ„ì— audio_prompterë¡œë¶€í„° ë‚˜ì˜¨ deltaë¥¼ ë”í•´ ì¤€ë‹¤.
#     """
#     def __init__(self, base_model_name="openai/whisper-large-v2", max_frames=3000, freeze_whisper=True):
#         super().__init__()
#         # 1) Whisper ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
#         self.whisper = WhisperForConditionalGeneration.from_pretrained(base_model_name)

#         # Whisper ì‚¬ì „í•™ìŠµ íŒŒë¼ë¯¸í„° ë™ê²° (ì˜µì…˜)
#         if freeze_whisper:
#             for param in self.whisper.parameters():
#                 param.requires_grad = False

#         # 2) AudioPrompter ì´ˆê¸°í™”
#         self.prompter = AudioPrompter(num_mel_bins=128, max_frames=max_frames)

#     def forward(self, input_features, labels=None, **kwargs):
#         """
#         - ğŸ¤—Transformers TrainerëŠ” model(**batch) í˜•íƒœë¡œ í˜¸ì¶œ.
#         - batch ë”•ì…”ë„ˆë¦¬ì—ëŠ” "input_features", "labels" ë“±ì´ ë“¤ì–´ìˆìŒ.
#         - input_features shape: (batch_size, 80, frames)
#         """

#         kwargs.pop("num_items_in_batch", None)
#         # ì˜¤ë””ì˜¤ í”„ë¡¬í”„íŠ¸ ì ìš©
#         prompted_features = self.prompter(input_features)  # (B,80,frames)

#         # Whisper forward
#         result = self.whisper(
#             input_features=prompted_features,
#             labels=labels,
#             **kwargs
#         )
#         return result


class AudioPromptingWhisper(WhisperForConditionalGeneration):
    def __init__(self, config, prompter_config=None, freeze_whisper=True):
        """
        Args:
            config: Whisper ëª¨ë¸ì˜ ì„¤ì • ê°ì²´.
            prompter_config: í”„ë¡¬í”„í„° ì„¤ì • (e.g., max_frames).
            freeze_whisper: Whisper íŒŒë¼ë¯¸í„°ë¥¼ freezeí• ì§€ ì—¬ë¶€.
        """
        super().__init__(config)
        # Prompter ì´ˆê¸°í™”
        self.prompter = AudioPrompter(num_mel_bins=128, max_frames=prompter_config.get("max_frames", 3000))

        # Whisper ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ freeze
        if freeze_whisper:
            for param in self.parameters():
                param.requires_grad = False

    def forward(self, input_features, labels=None, **kwargs):
        """
        Whisperì˜ forward ë©”ì„œë“œë¥¼ ì¬ì •ì˜í•˜ì—¬ Prompterë¥¼ ì ìš©.
        """
        # Prompterë¥¼ í†µí•´ delta ì ìš©
        prompted_features = self.prompter(input_features)
        # Whisperì˜ ì›ë˜ forward í˜¸ì¶œ
        return super().forward(input_features=prompted_features, labels=labels, **kwargs)

    # def generate(self, input_features, **kwargs):
    #     """
    #     Whisperì˜ generate ë©”ì„œë“œë¥¼ ì¬ì •ì˜í•˜ì—¬ Prompterë¥¼ ì ìš©.
    #     """
    #     # Prompterë¥¼ í†µí•´ delta ì ìš©
    #     prompted_features = self.prompter(input_features)
    #     # Whisperì˜ ì›ë˜ generate í˜¸ì¶œ
    #     return super().generate(input_features=prompted_features, **kwargs)
    
    def save_delta(self, path="audio_delta.pth"):
        """
        AudioPrompterì— í•™ìŠµëœ delta íŒŒë¼ë¯¸í„°ë§Œ ì €ì¥
        """
        torch.save(self.prompter.state_dict(), path)
        print(f"[AudioPromptingWhisper] Saved delta params to {path}")

    def load_delta(self, path="audio_delta.pth"):
        """
        ì €ì¥ëœ delta íŒŒë¼ë¯¸í„° ë¡œë“œ
        """
        self.prompter.load_state_dict(torch.load(path))
        print(f"[AudioPromptingWhisper] Loaded delta params from {path}")

    def gradient_checkpointing_enable(self, **kwargs):
        """
        ì¤‘ê°„ì— self.whisperê°€ PreTrainedModelì´ë©´ 
        whisper.gradient_checkpointing_enable() í˜¸ì¶œì„ ì¤‘ê³„
        """
        if hasattr(self.whisper, "gradient_checkpointing_enable"):
            self.whisper.gradient_checkpointing_enable(**kwargs)
        else:
            raise AttributeError("Whisper model does not support gradient_checkpointing_enable.")

    def gradient_checkpointing_disable(self):
        if hasattr(self.whisper, "gradient_checkpointing_disable"):
            self.whisper.gradient_checkpointing_disable()
        else:
            raise AttributeError("Whisper model does not support gradient_checkpointing_disable.")